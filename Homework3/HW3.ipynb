{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning in Medicine\n",
    "## BMSC-GA 4493, BMIN-GA 3007\n",
    "## Homework 3: RNNs\n",
    "\n",
    "Note 1: You can either work on HPC (recommended) or Google Colab for this homework.\n",
    "\n",
    "Note 2: If you need to write mathematical terms, you can type your answers in a Markdown Cell via LaTex\n",
    "See: <a href=\"https://stackoverflow.com/questions/13208286/how-to-write-latex-in-ipython-notebook\">here</a> if you have issues. To see basic LaTex notation see: <a href=\"https://en.wikibooks.org/wiki/LaTeX/Mathematics\">here</a>.\n",
    "\n",
    "\n",
    "Submission instruction: Upload your final jupyter notebook file, along with any figures that you may produce, in a zipped file named **netid_hw3** on Brightspace.\n",
    "\n",
    "**Submission deadline: Thursday April 14th 2022 11:59pm.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Literature Review: LSTM for ECG Signal Classification (Total points 20 + 5 bonus points)\n",
    "\n",
    "Read this paper:\n",
    "\n",
    "#### Yildirim, Ã–zal. \"A novel wavelet sequence based on deep bidirectional LSTM network model for ECG signal classification.\" Computers in biology and medicine 96 (2018): 189-202.\n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S0010482518300738?casa_token=XhS9KHU_fQoAAAAA:FGE7jkAK3I7HRf1yZNl8LA2snhUo1VgyEYdL7oXP1aI3jkBoqEc3dnvGtw1FEuNzWhnlAZ5X\n",
    "\n",
    "We are interested in understanding the task, the methods that is proposed in this publication, technical aspects of the implementation, and possible future work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1) (5 points)** After you read the full article, go back to section Materials and Method. You can find the two proposed Deep Learning architectures in the paper. Please briefly describe the model structure of one of the model, DBLSTM-WS. What are the layers, number of features input to the network and the output dimension of the model? You can ignore specifics parameters for the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2) (5 points)** What is the loss function for the two proposed models (same for both)? What are the evaluation criteria used by the authors for the models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3) (5 points)** The Wavelet Sequences (WS) layer serves as a data transformation for the input ECG sequence. Please briefly describe the details functionality of the layer. Please also describe conceptually the effect of number of layers for WS layer on the signal **(Hint: check the included figures in section 4)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4) (5 points)** Are there some data augmentation/regularization that authors have used? What are some techniques that could have been used but wasn't? Briefly provide some explanation of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5) (Bonus, maximum 5 points)**. What other architectures would you try? For maximum point, name one architecture, briefly explain your motivation for it and in a few sentences explain why the proposed changes might work better. Do some literature review on the application of such architecture in the medical field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Literature Review: Multi-Channel Fusion LSTM (20 points)\n",
    "\n",
    "Read this paper: \n",
    "\n",
    "\n",
    "#### Liu, S., Wang, X., Xiang, Y., Xu, H., Wang, H., & Tang, B. (2022). Multi-channel Fusion LSTM for Medical Event Prediction using HERs. \n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S1532046422000272?casa_token=ymhReMXUX2kAAAAA:pVqghdvoabg5Rkz4IKInVLeiMQNfc3nHc9Y71voIvdD7Ba3yfWOY5ME9Yx97mICi04IREy5b\n",
    "\n",
    "In this paper, the authors propose two models with multi-channel fusion of LSTM to handle heterogeneous electronic medical records. We are interested in understanding how different channels of input signals are processed and embedded with LSTM modules, and subsequently used for downstream tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1) (10 points)** After reading the paper, focus on Secion 3.2. Describe, with relevant formulas, how the fusion module works for an individual auxillary channel for Single-Belt Fusion model and Multi-Belt Fusion model. Please also conceptually describe the difference of the fusion modules in the two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2) (5 points)** Consider the Multi-Belt Fusion model. Briefly describe the model structure. Consider that the model is used for a prediction downstream task. Please also describe how the MBF embedding is used for the prediction task, along with the loss function used in the study **(hint: check Section 3.3)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3) (5 points)** The authors experimented the model using two datasets. Which criteria are used to evaluate the models? What is the best model according to the evaluation criteria?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 - Programming: Build Classifiers on Medical Transcriptions - Recurrent Neural Networks and Self Attention(60 points + 10 bonus points)\n",
    "\n",
    "Let's build some models now. In this homework, we will focus on a dataset which has around 5000 medical transcriptions and the corresponding medical specialty. The data is available <a href=\"https://www.kaggle.com/tboyle10/medicaltranscriptions\">here</a>.\n",
    "\n",
    "Here, we will focus on predicting top few classes of medical specialty, from the transcription text. <a href=\"https://github.com/nyumc-dl/BMSC-GA-4493-Spring2022/tree/main/lab6\">Lab 6</a> will be very useful here.\n",
    "\n",
    "**If you are working on Google Colab, please change the working directory path and run the next cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you are working on Google Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "path = ''  # Change this to your drive directory\n",
    "os.chdir(os.path.join('drive/MyDrive', path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1) (5 points)** Read the csv using Pandas. Select the top 6 frequent classes ('medical_specialty') from the data. Only keep the rows that belong to one of these classes in your data. Which classes are there, and how many rows do you have after this filteration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2) (5 points)** Now convert your data into train, test and validation set. Shuffle the rows, and split them with ratios of (train:60%, valid:20%, test:20%). Set the random seed to 2022. Please follow the steps from https://pytorch.org/docs/stable/notes/randomness.html to set all the seeds to make the results reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3) (5 points)** Create a function to create vocabulary from the training data. Only use the transcription column for this. Use the tokenization scheme of your choice and create a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4) (10 points)** Write a dataloader and collate function so that we can begin to train our networks! You can choose to use either the complete transcription text or fix a maximum length of transcription text as input for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5) (10 points)** Now you are ready to build your sequence classification model!\n",
    "\n",
    "First, Build a simple GRU model that takes as input the text indices from the vocabulary, and ends with a softmax over total number of classes. Use the embedding and hidden dimension of your choice. \n",
    "\n",
    "**Please train your model to reach at the least 50% accuracy on the test set.**\n",
    "\n",
    "At each epoch, compute and print **Average Cross Entropy loss** and **Accuracy** on both **train and validation set** \n",
    "\n",
    "Plot your validation and train loss over different epochs. \n",
    "\n",
    "Plot your validation and train accuracies over different epochs. \n",
    "\n",
    "Finally print accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.6) (25 points)** Now, let's finetune a sequence classification model based on BERT. Please install the Huggingface's Transformers library for this. Use the Pretrained 'bert-base-uncased' model for this problem. Please use the BERT tokenizer from the pretrained built for 'bert-base-uncased' model . Use the AdamW optimizer from the transformers library for optimization. Remember BERT uses Attention masks for input so you need to create a separate dataloader for BERT. Please keep in mind that BERT can handle maximum of 512 tokens.\n",
    "\n",
    "**Please finetune the model so that it reaches at least 60% accuracy on the test set.**\n",
    "\n",
    "The rest of your experimental setting should be the same as 3.5:\n",
    "\n",
    "At each epoch, compute and print **Average Cross Entropy loss** and **Accuracy** on both **train and validation set** \n",
    "\n",
    "Plot your validation and train loss over different epochs. \n",
    "\n",
    "Plot your validation and train accuracies over different epochs. \n",
    "\n",
    "Finally print accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.7) (Bonus maximum 10 points)** List 5 examples on the test set that BERT misclassified. Describe reasons identified for misclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
